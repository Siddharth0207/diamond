<!DOCTYPE html>
<html>

<head>
    <title>Voice Agent</title>
    <style>
        body {
            font-family: sans-serif;
            padding: 20px;
            background: #f0f0f0;
        }

        #chat {
            max-width: 600px;
            margin: auto;
            padding: 10px;
            background-color: #dff0d8;
            border-left: 4px solid #3c763d;
        }

        .message {
            background: white;
            padding: 10px;
            border-radius: 8px;
            margin: 5px 0;
        }

        .partial {
            opacity: 0.6;
            font-style: italic;
        }
    </style>
</head>

<body>
    <h2>Let me find you a Diamond for you!</h2>
    <div id="chat"></div> <!-- Container for displaying transcriptions -->

    <script>
        // Audio and WebSocket variables
        let audioContext, processor, input; // Audio processing variables
        let ws = null;
        function playStreamedTTS() {
            if (ttsAudioChunks.length === 0) return;

            const rawData = ttsAudioChunks.shift(); // get first chunk
            const audioBuffer = ttsAudioContext.createBuffer(1, rawData.length / 2, 22050);
            const channelData = audioBuffer.getChannelData(0);

            for (let i = 0; i < rawData.length; i += 2) {
                const sample = (rawData[i + 1] << 8) | (rawData[i] & 0xff); // Little-endian
                channelData[i / 2] = sample / 32768.0; // 16-bit PCM normalized
            }

            const source = ttsAudioContext.createBufferSource();
            source.buffer = audioBuffer;
            source.connect(ttsAudioContext.destination);
            source.start();

            // Schedule next chunk
            setTimeout(playStreamedTTS, audioBuffer.duration * 1000);
        }  // WebSocket connection for real-time transcription
        let isSpeaking = false; // Flag to track if user is currently speaking
        let vadTimer = null;    // Timer for Voice Activity Detection (VAD)
        let audioBuffer = [];   // Buffer to hold audio samples for sending
        let ttsAudioChunks = [];
        let ttsAudioContext = new (window.AudioContext || window.webkitAudioContext)({ sampleRate: 22050 });

        // Voice Activity Detection (VAD) and audio settings
        const VAD_SENSITIVITY = 0.01; // Threshold for detecting speech
        const VAD_TIMEOUT = 1000;     // Time (ms) to wait before considering speech ended
        const SAMPLE_RATE = 16000;    // Audio sample rate (Hz)

        // Append a message to the chat area
        function appendMessage(text, isPartial = false, id = null) {
            if (!text.trim()) return; // Skip empty/whitespace-only messages

            const chat = document.getElementById('chat');
            // If updating an existing partial message
            if (id && document.getElementById(id)) {
                document.getElementById(id).innerText = text;
                return;
            }
            // Create a new message div
            const div = document.createElement("div");
            div.className = "message" + (isPartial ? " partial" : "");
            div.innerText = text;   // Set the message text
            if (id) div.id = id;
            chat.appendChild(div);
            chat.scrollTop = chat.scrollHeight; // Scroll to bottom
        }

        // Convert Float32 audio samples to 16-bit PCM
        function floatTo16BitPCM(float32Array) {
            const int16 = new Int16Array(float32Array.length);
            for (let i = 0; i < float32Array.length; i++) {
                const s = Math.max(-1, Math.min(1, float32Array[i]));   // Clamp to [-1, 1]
                int16[i] = s < 0 ? s * 0x8000 : s * 0x7FFF; // Convert to 16-bit signed integer
            }
            return int16;
        }

        // Send audio data to the server via WebSocket
        function sendAudio(isFinal = false) {
            if (!ws || ws.readyState !== WebSocket.OPEN || audioBuffer.length === 0) return;    // Ensure WebSocket is open and buffer is not empty

            const pcm = new Int16Array(audioBuffer);
            ws.send(pcm.buffer);  // Send as ArrayBuffer
            if (isFinal) {
                ws.send(JSON.stringify({ final: true })); // Signal end of utterance
            }

            audioBuffer = []; // Clear buffer after sending
        }
        function speakQuestionAndListen(questionText) {
            const utterance = new SpeechSynthesisUtterance(questionText);
            utterance.lang = 'en-US';
            speechSynthesis.cancel(); // Important: reset any prior utterances
            speechSynthesis.speak(utterance);
        }

        async function startAudio() {
            const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
            audioContext = new AudioContext({ sampleRate: SAMPLE_RATE });
            input = audioContext.createMediaStreamSource(stream);
            processor = audioContext.createScriptProcessor(4096, 1, 1);
            input.connect(processor);
            processor.connect(audioContext.destination);

            processor.onaudioprocess = (e) => {
                const inputData = e.inputBuffer.getChannelData(0);
                const max = Math.max(...inputData.map(Math.abs));
                const pcm = floatTo16BitPCM(inputData);
                audioBuffer.push(...pcm);

                if (max > VAD_SENSITIVITY) {
                    if (!isSpeaking) isSpeaking = true;
                    clearTimeout(vadTimer);
                    vadTimer = setTimeout(() => {
                        isSpeaking = false;
                        sendAudio(true);
                    }, VAD_TIMEOUT);
                }
            };

            setInterval(() => {
                if (isSpeaking) sendAudio(false);
            }, 2000);

            connectWebSocket(); // connect WS after mic starts
        }

        function connectWebSocket() {
            ws = new WebSocket("ws://localhost:8000/api/v1/audio/ws/audio");

            ws.onopen = () => {
                console.log("WebSocket connected.");
            };

            ws.onmessage = (event) => {
                if (typeof event.data === "string") {
                    const data = JSON.parse(event.data);
                    if (data.type === "partial") {
                        appendMessage(data.text, true, "partial");
                    } else if (data.type === "final") {
                        document.getElementById("partial")?.remove();
                        appendMessage(data.text);
                    } else if (data.type === "entities") {
                        appendMessage(`Entities: ${JSON.stringify(data.data)}`);
                    } else if (data.type === "tts_end") {
                        console.log("TTS playback complete.");
                    }
                } else if (event.data instanceof Blob) {
                    // Handle raw audio chunk (binary)
                    const reader = new FileReader();
                    reader.onload = () => {
                        const pcmBytes = new Uint8Array(reader.result);
                        ttsAudioChunks.push(pcmBytes);
                        if (ttsAudioChunks.length === 1) {
                            playStreamedTTS(); // kick off playback if idle
                        }
                    };
                    reader.readAsArrayBuffer(event.data);
                }
            };

            ws.onclose = () => {
                console.warn("WebSocket closed. Retrying in 2s...");
                setTimeout(connectWebSocket, 2000);
            };

            ws.onerror = (err) => {
                console.error("WebSocket error:", err);
            };
        }

        window.onload = () => {
            startAudio();
            speakQuestionAndListen("What type of Diamond do you want?");
        };
        window.onbeforeunload = () => {
            if (ws && ws.readyState === WebSocket.OPEN) {
                ws.close(1000, "Page unload");
            }
        };

    </script>
</body>

</html>